{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GNNの実装を解読するために\n",
    "\n",
    "## GNNとはどのようなアーキテクチャなのか？\n",
    "\n",
    "GNN（Graph Neural Network）は、グラフ構造データを扱うためのニューラルネットワークの一種である。グラフはノード（頂点）とエッジ（辺）から構成される。物質への応用の場合、ノード特徴量としては、元素の種類などをone-hot encodingでベクトルに変換したものを初期値とすることが多い。エッジはノード間の関係性を表しており、物質の場合は「ある距離より近いノード間にはエッジがある」と設定する場合が多い。GNNは、エッジを介して、つながっている先のノードの情報を用いてノードの特徴量を更新していく点に特徴がある。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PaiNN\n",
    "PaiNN（Polarizable Interaction Neural Network）は、同変性をもたせたGNNアーキテクチャの一つである。同変性というのが何かというと\n",
    "\n",
    "$$\n",
    "Rf(x) = f(Rx)\n",
    "$$\n",
    "\n",
    "つまり、入力変数に何らかの作用（ここでは回転とか）をしたものを入力した結果と、もともとの変数を入力した結果に同じ作用をしたものが等しい、という性質である。物質の性質は、物質全体を回転させたり平行移動させたりしても変わらないため、GNNに同変性をもたせることは重要である。PaiNNでは、ノード特徴量としてスカラー量とベクトル量の両方を用いることで同変性を実現している。\n",
    "\n",
    "ベクトル量の特徴量を同変に保てる演算は内積、定数倍、外積、線形結合である。その一方、ニューラルネットワークで必須となる非線形変換には注意が必要である。\n",
    "単純にベクトルの要素ごとに非線形変換を掛けてしまうと、同変性が崩れてしまう。そこで、ゲート機構という方法がよく用いられる。\n",
    "これは、非線形変換（シグモイド関数やReLU, SiLUなど）への入力を、ベクトルの特徴量と別のベクトルとの内積によってスカラーにしたものし、その変換結果を使ってもとのベクトル特徴量をスケーリングするという方法である。こうすることで、ベクトル特徴量自体には非線形変換を直接かけないため、同変性を保つことができる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "さて、PaiNNで行われている特徴量の更新を見ていこう。PaiNNにおける最大の特徴はスカラーとベクトル、両方の特徴量を持っていることである。この２つの特徴量がお互いに影響を受けながら更新をされていく。ベクトル特徴量を持っている最大の優位性は、角度の情報が自然と入ることにある。\n",
    "スカラー特徴量の典型例はボンドの長さである。しかし、ボンドの長さをいくら集積してもボンドの角度の情報は直接的に得ることはできない。全結合型NNPでは角度情報を明示的に入れ込むような特徴量が用いられるが、角度の計算は$O(N^2)$のコストであるため、避けたい。一方、ボンドの向きに相当するベクトル特徴量を持っておくと、その総和の二乗を取れば角度情報が自然と入ることになる。この演算自体は$O(N)$である。\n",
    "そこで、特徴量が２系統になるが、どちらも$O(N)$で済む演算を使うことで、効率的かつ構造記述をリッチにするという戦略を取っている。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### スカラー特徴量のmessage passing\n",
    "PaiNNにおける入力は、原子量$Z_i$と位置ベクトル$\\mathbf{r}_i$である。各ノード$i$に対して、スカラー特徴量$\\mathbf{s}_i$とベクトル特徴量$\\mathbf{v}_i$を持つ。\n",
    "スカラー特徴量の初期値は$Z_i$の埋め込みから得られる。そのスカラー特徴量の更新につかうメッセージパッシングは以下のように定義される。\n",
    "\n",
    "$$\n",
    "\\Delta s_i^m =  \\sum_{j \\in \\mathcal{N}(i)} \\phi_s(s_j) W_s(r_{ij})\n",
    "$$\n",
    "\n",
    "この部分は、ほぼSchNetと一緒である。$r_{ij}$は原子iとｊの間の距離であり、$\\phi_s$の実態はニューラルネットワークである。\n",
    "ここで、$W_s(r_{ij})$は、以下のような原子間距離を展開して得られるベクトルの線形変換によって得られる「不変フィルター」と呼ばれるものである。\n",
    "$$\n",
    "e_n=\\sin \\left(\\frac{n\\pi}{r_{cut}} r_{ij}\\right)/r_{ij}\n",
    "$$\n",
    "展開の方法がSchNetとは少し異なる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "展開の部分はコードの以下の部分に相当する"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "def sinc_expansion(edge_dist: torch.Tensor, edge_size: int, cutoff: float):\n",
    "    \"\"\"\n",
    "    calculate sinc radial basis function:\n",
    "    \n",
    "    sin(n *pi*d/d_cut)/d\n",
    "    \"\"\"\n",
    "    n = torch.arange(edge_size, device=edge_dist.device) + 1\n",
    "    return torch.sin(edge_dist.unsqueeze(-1) * n * torch.pi / cutoff) / edge_dist.unsqueeze(-1)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ベクトル特徴量のmessage passing\n",
    "ベクトル特徴量に対するメッセージパッシングは以下のように行われる\n",
    "\n",
    "$$\n",
    "\\Delta \\mathbf{v}_i^m =  \\sum_{j \\in \\mathcal{N}(i)} \\mathbf{v}_j\\phi_{vv}(s_j) W_{vv}(r_{ij})\n",
    "+ \\sum_{j \\in \\mathcal{N}(i)} \\phi_{vs}(s_j) W_{vs}(r_{ij}) \\frac{\\mathbf{r}_{ij}}{r_{ij}}\n",
    "$$\n",
    "\n",
    "第一項は前述のゲート機構と同じアイデアで、ベクトルをスケール変換する演算に対応し、第２項は実はフィルター$W(r_{ij})$を微分したようなものとみなすことができる。これによって「同変な」フィルターを実現している。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "スカラー特徴量、ベクトル特徴量いずれにおいても\n",
    "$$\n",
    "\\phi(s_j) W(r_{ij})\n",
    "$$\n",
    "という形が共通している。なので、一斉に計算し、最後にその結果を三分割するような実装がされている。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "class MessageLayer(nn.Module):\n",
    "    #atomwise message passing\n",
    "    def __init__(self, natom_basis, n_radial, cutoff):\n",
    "        super(MessageLayer, self).__init__()\n",
    "        self.natom_basis=natom_basis\n",
    "        self.n_radial=n_radial\n",
    "        self.cutoff=cutoff\n",
    "        self.interaction_context_network=nn.Sequential(\n",
    "            nn.Linear(self.natom_basis, natom_basis),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(self.natom_basis, natom_basis*3),\n",
    "        )\n",
    "\n",
    "        \n",
    "        self.filter_network=nn.Sequential(\n",
    "            nn.Linear(self.n_radial,natom_basis*3),\n",
    "        )\n",
    "\n",
    "    def forward(self, q, mu, edge_index, edge_weight):\n",
    "        #q: scalar representation\n",
    "        #mu: vector representation\n",
    "\n",
    "        #edge_weightとしては原子間ベクトルがそのまま入っている\n",
    "        \n",
    "        #message passing\n",
    "        #スカラー特徴量を変換\n",
    "        x = self.interaction_context_network(q)\n",
    "        distances=torch.norm(edge_weight, dim=-1)\n",
    "        #単位ベクトルを取得\n",
    "        directions=edge_weight/distances.unsqueeze(-1)\n",
    "        \n",
    "        #ここで展開をおこなっている\n",
    "        basis_fn=sinc_expansion(distances, self.n_radial, self.cutoff)\n",
    "        cutoff=cosine_cutoff(distances, self.cutoff).unsqueeze(-1)\n",
    "        #カットオフの導入\n",
    "        filter_Wij=self.filter_network(basis_fn)*cutoff\n",
    "        \n",
    "        #ペア情報の取得\n",
    "        idx_i=edge_index[0]\n",
    "        idx_j=edge_index[1]\n",
    "\n",
    "        #相手側のインデックスを取得\n",
    "        xj=x[idx_j]\n",
    "        muj=mu[idx_j]\n",
    "\n",
    "        #psi*Wの計算\n",
    "        x=filter_Wij*xj\n",
    "\n",
    "        #psi_s, psi_vv, psi_vsに分割\n",
    "        dq, dmuR, dmumu=torch.split(x, self.natom_basis, dim=-1)\n",
    "\n",
    "        #aggregation\n",
    "        index=idx_i.unsqueeze(1).expand_as(dq)\n",
    "        \n",
    "        q_update=torch.zeros_like(q,device=q.device)\n",
    "        q_update=torch.scatter_add(q_update, 0, index, dq)\n",
    "\n",
    "        #dmuRはpsi_vsに相当、dmumuはpsi_vvに相当\n",
    "        dmuR = dmuR.unsqueeze(1)\n",
    "        dmumu = dmumu.unsqueeze(1)\n",
    "        dmu = dmuR * directions[..., None] + dmumu * muj\n",
    "\n",
    "        index=idx_i.unsqueeze(-1).unsqueeze(-1).expand_as(dmu)\n",
    "        \n",
    "        mu_update=torch.zeros_like(mu,device=mu.device)\n",
    "        mu_update = torch.scatter_add(mu_update, 0, index, dmu)\n",
    "        \n",
    "        q=q+q_update\n",
    "        mu=mu+mu_update\n",
    "\n",
    "        return q, mu\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "interaction_context_networkが$\\phi(s_j)$に対応し、filter_networkが$W(r_{ij})$に対応している。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update layer\n",
    "作成したメッセージを用いて、ノードの特徴量を更新する際にもいくつかの工夫が用いられている。更新の際には非線形変換が入るのが通例なので、ここでゲート機構を用いる。ゲート機構には、ベクトル特徴量をスカラーに落とすための内積が用いられている。その内積に使う行列を、ベクトル特徴量に対する線形変換として学習させている。\n",
    "\n",
    "その際、２つの行列を生成し、それらを組み合わせて使うことで、うまくスカラー特徴量と、ベクトル特徴量を結びつけることができる。\n",
    "生成される行列２つをそれぞれ$U,V$と表記し、スカラー特徴量、ベクトル特徴量のUpdateを以下のように定義する。\n",
    "\n",
    "$$\n",
    "\\Delta s_i^u = a_{ss} (s_i, ||V\\mathbf{v}_i||) +a_{sv} (s_i, ||V\\mathbf{v}_i||) <U\\mathbf{v}_i, V\\mathbf{v}_i>\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\Delta u_i^u = a_{vv} (s_i, ||V\\mathbf{v}_i||) U\\mathbf{v}_i\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ここでもやはり共通した構造$a(s_i, ||V\\mathbf{v}_i||)$が出てきており、これの実態は全結合NNなので、まとめて処理して出力を分割するような方法を取る"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "対応する実装部分は以下のようになっている"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "class UpdateLayer(nn.Module):\n",
    "    def __init__(self, natom_basis, epsilon):\n",
    "        \"\"\"\n",
    "        updating scaler representation using vector representation\n",
    "        \n",
    "        natom_basis: embedding atom type dimension, dimension of scalar representation\n",
    "        epsilon: small value to avoid zero division\n",
    "        \"\"\"\n",
    "        super(UpdateLayer, self).__init__()\n",
    "        self.register_buffer(\"epsilon\", torch.tensor(epsilon))\n",
    "        self.natom_basis=natom_basis\n",
    "        \n",
    "        #s_i,Vv_iの結合したものを受け取るので最初の次元がself.natom_basis*2\n",
    "        self.intraatomic_context_net=nn.Sequential(\n",
    "            nn.Linear(self.natom_basis*2, self.natom_basis),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(self.natom_basis, self.natom_basis*3),\n",
    "        )\n",
    "        self.mu_channel_mix=nn.Sequential( \n",
    "            nn.Linear(self.natom_basis, self.natom_basis*2),\n",
    "        )\n",
    "\n",
    "    def forward(self, q, mu):\n",
    "        \"\"\"\n",
    "        updating scaler representation using vector representation\n",
    "        \n",
    "        q: scalar representation\n",
    "        mu: vector representation\n",
    "        \"\"\"\n",
    "        mu_mix=self.mu_channel_mix(mu)\n",
    "        mu_V,mu_W=torch.split(mu_mix, self.natom_basis, dim=-1)\n",
    "        mu_Vn = torch.sqrt(torch.sum(mu_V**2, dim=-2, keepdim=False) + self.epsilon)\n",
    "        \n",
    "        ctx = torch.cat([q, mu_Vn], dim=-1)\n",
    "        x = self.intraatomic_context_net(ctx)\n",
    "        dq_intra, dmu_intra, dqmu_intra = torch.split(x, self.natom_basis, dim=-1)\n",
    "\n",
    "        dmu_intra = dmu_intra.unsqueeze(1) * mu_W\n",
    "\n",
    "        dqmu_intra = dqmu_intra * torch.sum(mu_V * mu_W, dim=1, keepdim=False)\n",
    "\n",
    "        q = q + dq_intra + dqmu_intra\n",
    "        mu = mu + dmu_intra\n",
    "        \n",
    "        return q,mu\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "U,Vを作成するのがmu_channel_mixに対応し、intraatomic_context_netが$a(s_i, ||V\\mathbf{v}_i||)$に対応している。\n",
    "ノルムが0になるのを防ぐためにepsilonが加えられている点に注意。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
